#!/usr/bin/env python3
"""
ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ë²ˆì—­ íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ íë¦„ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

ì‚¬ì „ ì¤€ë¹„:
    ./setup/create_env.sh  # í•„ìˆ˜ (ìµœì´ˆ 1íšŒ)

ì‚¬ìš©ë²•:
    uv run python test_workflow.py                              # ë‹¨ì¼ í…ŒìŠ¤íŠ¸
    uv run python test_workflow.py --input examples/single/ui.json
    uv run python test_workflow.py --batch --input examples/batch/mixed.json

ì˜µì…˜:
    --input FILE        ì…ë ¥ JSON íŒŒì¼ (ê¸°ë³¸: examples/single/default.json)
    --batch             ë°°ì¹˜ ëª¨ë“œ (ëª¨ë“  í…ŒìŠ¤íŠ¸ í•­ëª© ì‹¤í–‰)
    --max-regen N       ìµœëŒ€ ì¬ìƒì„± íšŸìˆ˜ (ê¸°ë³¸: 1)
    --session-id ID     OTEL ì„¸ì…˜ ID ì§€ì •
"""

# =============================================================================
# ì˜ì¡´ì„±
# =============================================================================
import asyncio
import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import List

sys.path.insert(0, str(Path(__file__).parent))

from src.models import TranslationUnit
from src.graph import TranslationWorkflowGraph, WorkflowConfig
from src.utils.pricing import calculate_workflow_cost
from src.utils.result_formatter import (
    format_workflow_result,
    calculate_batch_stats,
    save_batch_summary,
)

# =============================================================================
# OTEL ì„¤ì • (ì„ íƒì )
# - ì„¤ì¹˜ë¨: CloudWatchë¡œ íŠ¸ë ˆì´ìŠ¤ ì „ì†¡
# - ë¯¸ì„¤ì¹˜: ë”ë¯¸ ì„¸ì…˜ìœ¼ë¡œ ëŒ€ì²´ (ê¸°ëŠ¥ ì •ìƒ ì‘ë™)
# =============================================================================
try:
    from src.utils.strands_utils import observability_session
    OTEL_AVAILABLE = True
except ImportError:
    OTEL_AVAILABLE = False
    from contextlib import contextmanager
    @contextmanager
    def observability_session(**kwargs):
        yield {"session_id": kwargs.get("session_id") or "no-otel"}

# =============================================================================
# ì„¤ì •
# =============================================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ë¶ˆí•„ìš”í•œ ë¡œê·¸ ì–µì œ
logging.getLogger("botocore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("src.utils.strands_utils").setLevel(logging.WARNING)
logging.getLogger("strands.telemetry.metrics").setLevel(logging.WARNING)

RESULTS_DIR = Path(__file__).parent / "results"    # ê²°ê³¼ ì €ì¥ ìœ„ì¹˜
EXAMPLES_DIR = Path(__file__).parent / "examples"  # ì˜ˆì œ ì…ë ¥ íŒŒì¼


# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================
def log_header(*lines: str) -> None:
    """êµ¬ë¶„ì„ ìœ¼ë¡œ ê°ì‹¼ í—¤ë” ì¶œë ¥"""
    logger.info(f"\n{'='*60}")
    for line in lines:
        logger.info(line)
    logger.info("="*60)


def log_section(title: str) -> None:
    """ì„¹ì…˜ ì œëª© ì¶œë ¥"""
    logger.info(f"\n--- {title} ---")


def print_json_block(title: str, data: dict, summary_only: bool = False) -> None:
    """JSON ë¸”ë¡ ì¶œë ¥ (summary_only=Trueë©´ details ì œì™¸)"""
    print(f"\n{'='*60}")
    print(title)
    print("="*60)
    if summary_only and "details" in data:
        summary = {k: v for k, v in data.items() if k != "details"}
        print(json.dumps(summary, ensure_ascii=False, indent=2))
    else:
        print(json.dumps(data, ensure_ascii=False, indent=2))


def load_test_units(json_path: Path = None) -> List[TranslationUnit]:
    """JSON íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    if json_path is None:
        json_path = EXAMPLES_DIR / "single" / "default.json"

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    return [TranslationUnit(**item) for item in data]


# =============================================================================
# í…ŒìŠ¤íŠ¸ ë°ì´í„°
# - config/test_data.yaml ì—ì„œ ë¡œë“œ (ê¸°ë³¸ê°’)
# - --input ì˜µì…˜ìœ¼ë¡œ ë‹¤ë¥¸ íŒŒì¼ ì§€ì • ê°€ëŠ¥
# - ê° í•­ëª©: key, source_text, source_lang, target_lang, glossary ë“±
# =============================================================================


def get_timestamp() -> str:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (yyyy-mm-dd-hh-mm-ss-mms)"""
    now = datetime.now()
    return now.strftime("%Y-%m-%d-%H-%M-%S") + f"-{now.microsecond // 1000:03d}"


def save_result(result: dict, run_dir: Path) -> Path:
    """ë‹¨ì¼ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    key = result["unit"].key
    file_path = run_dir / f"{key}.json"

    output = format_workflow_result(result)

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    return file_path


def log_batch_stats(stats: dict) -> None:
    """ë°°ì¹˜ í†µê³„ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥"""
    log_section("ë°°ì¹˜ ê²°ê³¼")
    logger.info(f"  ë°œí–‰: {stats['published']}/{stats['total']}")
    logger.info(f"  ê±°ë¶€: {stats['rejected']}/{stats['total']}")
    logger.info(f"  ê²€ìˆ˜ëŒ€ê¸°: {stats['pending']}/{stats['total']}")
    logger.info(f"  ì‹¤íŒ¨: {stats['failed']}/{stats['total']}")


def log_single_result(result: dict) -> None:
    """ë‹¨ì¼ ë²ˆì—­ ê²°ê³¼ë¥¼ íŠ¸ë¦¬ í˜•íƒœë¡œ ì¶œë ¥"""
    m = result.get('metrics')
    total_ms = m.total_latency_ms if m else 0
    attempt = result.get('attempt_count', 1)
    state = result['workflow_state'].value

    # ìƒíƒœ ì•„ì´ì½˜
    icon = {"published": "âœ…", "pending_review": "âš ï¸", "rejected": "âŒ", "failed": "ğŸ’¥"}.get(state, "ğŸ”„")

    # í—¤ë”
    print(f"\n{icon} ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ({total_ms/1000:.1f}s)")

    # ë²ˆì—­
    if 'translation_result' in result:
        tr = result['translation_result']
        print(f"â”œâ”€ ë²ˆì—­: {tr.latency_ms}ms")
        print(f"â”‚   â””â”€ {tr.translation[:60]}{'...' if len(tr.translation) > 60 else ''}")

    # ì—­ë²ˆì—­
    if 'backtranslation_result' in result:
        bt = result['backtranslation_result']
        print(f"â”œâ”€ ì—­ë²ˆì—­: {bt.latency_ms}ms")
        print(f"â”‚   â””â”€ {bt.backtranslation[:60]}{'...' if len(bt.backtranslation) > 60 else ''}")

    # í‰ê°€
    if 'agent_results' in result:
        eval_latency = m.evaluation_latency_ms if m else 0
        print(f"â”œâ”€ í‰ê°€: {eval_latency}ms")
        agents = result['agent_results']
        for i, ar in enumerate(agents):
            is_last = (i == len(agents) - 1)
            prefix = "â”‚   â””â”€" if is_last else "â”‚   â”œâ”€"
            score_icon = "âœ“" if ar.score >= 4 else ("â–³" if ar.score == 3 else "âœ—")
            print(f"{prefix} {ar.agent_name}: {ar.score} {score_icon}")
            # ì´ìŠˆê°€ ìˆìœ¼ë©´ í‘œì‹œ
            if ar.issues and ar.score < 4:
                issue_prefix = "â”‚       â””â”€" if is_last else "â”‚   â”‚   â””â”€"
                print(f"{issue_prefix} {ar.issues[0][:50]}...")

    # íŒì • (ì‹œë„ íˆìŠ¤í† ë¦¬ í¬í•¨)
    if 'attempt_history' in result and len(result['attempt_history']) > 1:
        print(f"â””â”€ íŒì • ({attempt}íšŒ ì‹œë„)")
        history = result['attempt_history']
        for i, h in enumerate(history):
            is_last = (i == len(history) - 1)
            prefix = "    â””â”€" if is_last else "    â”œâ”€"
            scores_str = ", ".join(f"{k}:{v}" for k, v in h['scores'].items())
            print(f"{prefix} [ì‹œë„ {h['attempt']}] {h['verdict']} ({scores_str})")
            if h['message'] and is_last:
                print(f"        â””â”€ {h['message']}")
    elif 'gate_decision' in result:
        gd = result['gate_decision']
        print(f"â””â”€ íŒì •: {gd.verdict.value}")
        if gd.message:
            print(f"    â””â”€ {gd.message}")

    # ë¹„ìš©
    if m:
        cost = calculate_workflow_cost(m.token_usage)
        print(f"\nğŸ’° ë¹„ìš©: ${cost.total_cost:.4f} | í† í°: {m.token_usage['input']:,}+{m.token_usage['output']:,}")

    # ì˜¤ë¥˜
    if 'error' in result:
        print(f"\nâŒ ì˜¤ë¥˜: {result['error']}")


# =============================================================================
# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# =============================================================================
async def test_single_translation(unit: TranslationUnit, config: WorkflowConfig) -> dict:
    """ë‹¨ì¼ ë²ˆì—­ í…ŒìŠ¤íŠ¸"""
    log_header(
        f"í…ŒìŠ¤íŠ¸: {unit.key}",
        f"ì›ë¬¸: {unit.source_text[:50]}...",
        f"ëŒ€ìƒ ì–¸ì–´: {unit.target_lang}"
    )

    graph = TranslationWorkflowGraph(config)
    result = await graph.run(unit)

    log_single_result(result)

    # ê²°ê³¼ ì €ì¥
    timestamp = get_timestamp()
    run_dir = RESULTS_DIR / "single" / timestamp
    run_dir.mkdir(parents=True, exist_ok=True)

    file_path = save_result(result, run_dir)
    logger.info(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {file_path}")

    # JSON ì¶œë ¥ (ìš”ì•½ë§Œ)
    output_dict = format_workflow_result(result)
    print_json_block("ğŸ“„ Result JSON:", output_dict, summary_only=True)

    return result


async def test_batch_translation(units: list, config: WorkflowConfig, concurrency: int = 2) -> list:
    """ë°°ì¹˜ ë²ˆì—­ í…ŒìŠ¤íŠ¸"""
    log_header(f"ë°°ì¹˜ í…ŒìŠ¤íŠ¸: {len(units)}ê°œ í•­ëª©, ë™ì‹œì„± {concurrency}")

    graph = TranslationWorkflowGraph(config)
    results = await graph.run_batch(units, concurrency=concurrency)

    # í†µê³„
    stats = calculate_batch_stats(results)
    log_batch_stats(stats)

    # ê²°ê³¼ ì €ì¥
    timestamp = get_timestamp()
    run_dir = RESULTS_DIR / "batch" / timestamp
    run_dir.mkdir(parents=True, exist_ok=True)

    for result in results:
        save_result(result, run_dir)

    summary_path = save_batch_summary(results, run_dir)
    logger.info(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {run_dir}")
    logger.info(f"ğŸ“Š ìš”ì•½: {summary_path}")

    # ê° ê²°ê³¼ JSON ì¶œë ¥ (ìš”ì•½ë§Œ)
    for result in results:
        output_dict = format_workflow_result(result)
        print_json_block(f"ğŸ“„ Result JSON ({result['unit'].key}):", output_dict, summary_only=True)

    # ìš”ì•½ JSON ì¶œë ¥
    with open(summary_path, "r", encoding="utf-8") as f:
        summary = json.load(f)
    print_json_block("ğŸ“Š Summary JSON:", summary)

    return results


# =============================================================================
# ë©”ì¸ ì§„ì…ì 
# =============================================================================
async def main():
    """ëª…ë ¹ì¤„ ì¸ìë¥¼ íŒŒì‹±í•˜ê³  ì ì ˆí•œ í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰"""
    parser = argparse.ArgumentParser(description="ë²ˆì—­ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸")
    parser.add_argument("--input", type=str, help="ì…ë ¥ YAML íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--batch", action="store_true", help="ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--max-regen", type=int, default=1, help="ìµœëŒ€ ì¬ìƒì„± íšŸìˆ˜ (ê¸°ë³¸: 1)")
    parser.add_argument("--session-id", type=str, help="ì»¤ìŠ¤í…€ ì„¸ì…˜ ID")
    parser.add_argument("--debug", action="store_true", help="DEBUG ë¡œê·¸ ë ˆë²¨ í™œì„±í™” (í”„ë¡¬í”„íŠ¸ ì¶œë ¥)")
    args = parser.parse_args()

    # DEBUG ëª¨ë“œ ì„¤ì •
    if args.debug:
        logging.getLogger("src.tools").setLevel(logging.DEBUG)
        # strands ë‚´ë¶€ ë¡œê·¸ëŠ” ìˆ¨ê¹€
        logging.getLogger("strands").setLevel(logging.WARNING)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    input_path = Path(args.input) if args.input else None
    test_units = load_test_units(input_path)

    logger.info(f"OTEL: {'ENABLED' if OTEL_AVAILABLE else 'DISABLED'}")

    # ì›Œí¬í”Œë¡œìš° ì„¤ì •
    config = WorkflowConfig(
        max_regenerations=args.max_regen,
        num_candidates=1,
        enable_backtranslation=True,
        timeout_seconds=120
    )

    # Observability ì„¸ì…˜ìœ¼ë¡œ ì‹¤í–‰
    workflow_name = "batch" if args.batch else "single"

    with observability_session(
        session_id=args.session_id,
        workflow_name=f"translation-{workflow_name}",
        metadata={"test_mode": workflow_name}
    ) as session:
        logger.info(f"Session ID: {session['session_id']}")

        if args.batch:
            await test_batch_translation(test_units, config)
        else:
            await test_single_translation(test_units[0], config)

    if OTEL_AVAILABLE:
        logger.info("View traces: https://console.aws.amazon.com/cloudwatch/home#gen-ai-observability")


if __name__ == "__main__":
    asyncio.run(main())
